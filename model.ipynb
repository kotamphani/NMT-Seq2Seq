{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random as r\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Import custom utilities\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155798\n",
      "155798\n"
     ]
    }
   ],
   "source": [
    "# Load data from the dataset\n",
    "path = 'english_telugu_data.txt'\n",
    "english_sentences, telugu_sentences = load_data(path)\n",
    "print(len(english_sentences))\n",
    "print(len(telugu_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "english_sentences = preprocess(english_sentences)\n",
    "telugu_sentences = preprocess(telugu_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['అతని కాళ్ళు పొడవుగా ఉన్నాయి', 'టామ్ ఫ్రెంచ్ మాట్లాడటం ఎలా నేర్పించారు']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "telugu_sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his legs are long',\n",
       " 'who taught tom how to speak french',\n",
       " 'i swim in the sea every day',\n",
       " 'tom popped into the supermarket on his way home to buy some milk',\n",
       " 'smoke filled the room',\n",
       " 'tom and mary understood each other',\n",
       " 'many men want to be thin too',\n",
       " 'we need three cups',\n",
       " 'i warned tom not to come here',\n",
       " 'you two may leave']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>155798.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.161979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.435330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>527.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count  155798.000000\n",
       "mean       29.161979\n",
       "std        11.435330\n",
       "min         2.000000\n",
       "25%        21.000000\n",
       "50%        27.000000\n",
       "75%        35.000000\n",
       "max       527.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the length of each sentence\n",
    "lengths = [len(sentence) for sentence in english_sentences]\n",
    "lengths = pd.DataFrame(lengths)\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ఏ పురుషుడైనా వివాహం చేసుకోవడానికి మరియు దీనికి విరుద్ధంగా సరైన స్త్రీ ఈ ప్రపంచంలో ఖచ్చితంగా ఉన్నారనడంలో సందేహం లేదు'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the sentence with length 527 from the list\n",
    "english_sentences.pop(lengths[lengths[0] == 527].index[0])\n",
    "telugu_sentences.pop(lengths[lengths[0] == 527].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " len(english_vocab): 14225\n",
      " len(telugu_vocab): 38835\n"
     ]
    }
   ],
   "source": [
    "# Create vocabularies for Telugu and English words\n",
    "telugu_vocab = get_vocab(telugu_sentences)\n",
    "english_vocab = get_vocab(english_sentences)\n",
    "\n",
    "# Add special tokens to the vocabularies\n",
    "telugu_vocab  = ['<pad>','<unk>','<sos>','<eos>'] + telugu_vocab\n",
    "english_vocab = ['<pad>','<unk>','<sos>','<eos>'] + english_vocab\n",
    "\n",
    "print(f' len(english_vocab): {len(english_vocab)}')\n",
    "print(f' len(telugu_vocab): {len(telugu_vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_itow =  14225\n",
      "telugu_itow =  38835\n",
      "english_wtoi =  14225\n",
      "telugu_wtoi =  38835\n"
     ]
    }
   ],
   "source": [
    "# Create mappings between words and indices for both languages\n",
    "english_itow = {i: w for i, w in enumerate(english_vocab)}\n",
    "english_wtoi = {w:i for i, w in english_itow.items()}\n",
    " \n",
    "telugu_itow = {i: w for i, w in enumerate(telugu_vocab)}\n",
    "telugu_wtoi = {w:i for i, w in telugu_itow.items()}\n",
    "\n",
    "print(\"english_itow = \", len(english_itow)) \n",
    "print(\"telugu_itow = \", len(telugu_itow))\n",
    "print(\"english_wtoi = \", len(english_wtoi))\n",
    "print(\"telugu_wtoi = \", len(telugu_wtoi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140217, 140217, 15580, 15580)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(english_sentences, telugu_sentences, test_size=0.1)\n",
    "len(X_train), len(Y_train), len(X_val), len(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140217 140217 140217\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input and output of train data\n",
    "X_train = tokenize_input(X_train, english_wtoi)\n",
    "Y_train, Y_train_shifted = tokenize_output(Y_train, telugu_wtoi)\n",
    "print(len(X_train), len(Y_train), len(Y_train_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15580 15580 15580\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input and output of val data\n",
    "X_val = tokenize_input(X_val, english_wtoi)\n",
    "Y_val, Y_val_shifted = tokenize_output(Y_val, telugu_wtoi)\n",
    "print(len(X_val), len(Y_val), len(Y_val_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign buckets based on sentence lengths\n",
    "train_buckets = assign_buckets(X_train, [8, 16, 32, 64], [256, 128, 64, 8])\n",
    "val_buckets = assign_buckets(X_val, [8, 16, 32, 64], [256, 128, 64, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together\n",
    "train_items = (X_train, Y_train, Y_train_shifted, train_buckets)\n",
    "val_items = (X_val, Y_val, Y_val_shifted, val_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_items, shuffle = True, verbose = False):\n",
    "    \n",
    "    X, Y, Y_shifted, buckets = data_items\n",
    "    \n",
    "    bin = r.choices(list(buckets.keys()), weights= [0.25, 0.25, 0, 0])[0]   # made probs of sequence length more than 16 to be 0 because of less data prone to overfitting.\n",
    "        \n",
    "    indexes = buckets[bin]\n",
    "    batch_size = bin[2]\n",
    "    \n",
    "    if shuffle:\n",
    "        r.shuffle(indexes)\n",
    "    \n",
    "    buffer_x = [0]*batch_size\n",
    "    buffer_y = [0]*batch_size\n",
    "    buffer_y_shifted = [0]*batch_size\n",
    "    \n",
    "    max_length_x = bin[1]\n",
    "    max_length_y = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        buffer_x[i] = X[indexes[i]]\n",
    "        buffer_y[i] = Y[indexes[i]]\n",
    "        buffer_y_shifted[i] = Y_shifted[indexes[i]]\n",
    "        max_length_y = max(len(Y[indexes[i]]), max_length_y)\n",
    "    \n",
    "    max_length_y = 2**int(np.ceil((np.log2(max_length_y))))\n",
    "\n",
    "    \n",
    "    for index, (x, y, y_shifted) in enumerate(zip(buffer_x, buffer_y, buffer_y_shifted)):\n",
    "        pad_length_x = max_length_x - len(x)\n",
    "        pad_length_y = max_length_y - len(y)\n",
    "        \n",
    "        buffer_x[index] = x + [english_wtoi['<pad>']] * pad_length_x\n",
    "        buffer_y[index] = y + [telugu_wtoi['<pad>']] * pad_length_y\n",
    "        buffer_y_shifted[index] = y_shifted + [telugu_wtoi['<pad>']] * pad_length_y\n",
    "    \n",
    "    \n",
    "    buffer_x = torch.tensor(buffer_x)\n",
    "    buffer_y = torch.tensor(buffer_y)\n",
    "    buffer_y_shifted = torch.tensor(buffer_y_shifted)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(buffer_x.shape, buffer_y.shape, buffer_y_shifted.shape)\n",
    "    \n",
    "    return buffer_x, buffer_y, buffer_y_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 8]) torch.Size([256, 8]) torch.Size([256, 8])\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, train_target_shifted = data_generator(train_items,shuffle = True, verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16]) torch.Size([128, 16]) torch.Size([128, 16])\n"
     ]
    }
   ],
   "source": [
    "val_input, val_target, val_target_shifted = data_generator(val_items ,shuffle = True, verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 8, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim):        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size= hidden_dim,num_layers=2, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.LSTM(x)[0]\n",
    "        return x\n",
    "\n",
    "# Just for testing\n",
    "test_encoder = Encoder(len(english_wtoi), embedding_dim=10, hidden_dim=1)\n",
    "encoder_output = test_encoder.forward(train_input)# (batch_size, input_sequence_length, hidden_dim)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PreAttentionDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PreAttentionDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(target_vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers = 2, batch_first=True)\n",
    "        \n",
    "    def forward(self, y):\n",
    "        y = self.embedding(y)\n",
    "        y = self.LSTM(y)[0]\n",
    "        return y\n",
    "\n",
    "# For testing\n",
    "test_decoder = PreAttentionDecoder(len(telugu_wtoi), embedding_dim = 10, hidden_dim =1)\n",
    "decoder_output = test_decoder.forward(val_target_shifted) # (batch_size, target_shifted_length, hidden_dim)\n",
    "decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_output, decoder_output, input):\n",
    "\n",
    "    keys = encoder_output\n",
    "    values = encoder_output\n",
    "    \n",
    "    queries = decoder_output\n",
    "    \n",
    "    mask = (input == 0)\n",
    "    mask = mask.unsqueeze(-2)\n",
    "    mask = mask.repeat(4, decoder_output.shape[1], 1)\n",
    "    \n",
    "    #mask = mask.view((mask.shape[0] * 1, 1, mask.shape[1]))  \n",
    "    #mask = mask + torch.zeros((1, decoder_output.shape[1], 1))\n",
    "\n",
    "    # mask shape = (batch_size * num_heads_attention, target_shifted_length, input_sequence_length) \n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, hidden_dim, target_vocab_size):\n",
    "        super(Translator, self).__init__()\n",
    "        self.encoder = Encoder(input_vocab_size, embedding_dim, hidden_dim)\n",
    "        self.predecoder = PreAttentionDecoder(target_vocab_size, embedding_dim, hidden_dim) \n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads = 4, batch_first = True)\n",
    "        self.decoder = nn.LSTM(hidden_dim, hidden_dim, num_layers = 2, batch_first = True)\n",
    "        self.linear = nn.Linear(hidden_dim, target_vocab_size)\n",
    "        \n",
    "    def forward(self, input, target_shifted):\n",
    "        q, k, v, m = prepare_attention_input(self.encoder(input), self.predecoder(target_shifted), input)\n",
    "        attn_output, _ = self.attention(q, k, v, attn_mask = m)\n",
    "        attn_output = attn_output + q\n",
    "        attn_output = nn.LayerNorm([attn_output.shape[1], attn_output.shape[2]])(attn_output)\n",
    "        y = self.decoder(attn_output)[0]\n",
    "        logits = self.linear(y)\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(train_items, val_items, epochs, NNmodel):\n",
    "    interval = epochs // 10\n",
    "    optimizer = optim.AdamW(NNmodel.parameters())\n",
    "    NNmodel.train()\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        input, target, target_shifted = data_generator(train_items)\n",
    "        logits = NNmodel.forward(input, target_shifted)\n",
    "        logits = logits.view(-1, len(telugu_wtoi))\n",
    "        target = target.view(-1)\n",
    "        loss = F.cross_entropy(logits, target, ignore_index=telugu_wtoi['<pad>'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % interval == 0:\n",
    "            val_input, val_target, val_target_shifted = data_generator(val_items)\n",
    "            val_logits = NNmodel.forward(val_input, val_target_shifted)\n",
    "            val_logits = val_logits.view(-1, len(telugu_wtoi))\n",
    "            val_target = val_target.view(-1)\n",
    "            val_loss = F.cross_entropy(val_logits, val_target, ignore_index=telugu_wtoi['<pad>'])\n",
    "            print(f'Loss at epoch {epoch} ====> train: {loss.item():.4f}, val: {val_loss.item():.4f}')\n",
    "        \n",
    "    \n",
    "    return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(NNmodel):\n",
    "    input, target, target_shifted = data_generator(val_items)\n",
    "    logits = NNmodel.forward(input, target_shifted)\n",
    "\n",
    "    preds = F.softmax(logits, dim = 2)\n",
    "    preds = torch.argmax(preds, dim = 2)\n",
    "\n",
    "    mask = target != telugu_wtoi['<pad>']\n",
    "    accuracy = torch.sum(preds == target)/float(torch.sum(mask)) *100\n",
    "\n",
    "    logits = logits.view(-1, len(telugu_wtoi))\n",
    "    target = target.view(-1)\n",
    "\n",
    "    loss = F.cross_entropy(logits, target, ignore_index = telugu_wtoi['<pad>'])\n",
    "\n",
    "    print(f'Loss = {loss:.2f}, Accuracy = {accuracy:.2f}')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsoftmax_sample(log_probs, temperature=1.0, dim = 0): \n",
    "  \"\"\"Returns a sample from a log-softmax output, with temperature.\n",
    "\n",
    "  Args:\n",
    "    log_probs: Logarithms of probabilities (often coming from LogSofmax)\n",
    "    temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)\n",
    "  \"\"\"\n",
    "  # This is equivalent to sampling from a softmax with temperature.\n",
    "  u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
    "  g = torch.tensor(-np.log(-np.log(u)))\n",
    "  return torch.argmax(log_probs + g * temperature, dim = dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def next_symbol(NNmodel, input_token, cur_output_tokens):\n",
    "    \n",
    "    token_length = len(cur_output_tokens)\n",
    "    padded_length = 2**int(np.ceil((np.log2(token_length))))\n",
    "    padded = torch.tensor(cur_output_tokens + [0]*(padded_length - token_length))\n",
    "    padded = padded.view(1, padded_length)\n",
    "    \n",
    "    logits = NNmodel(input_token, padded)\n",
    "    \n",
    "    prob_last_token = F.softmax(logits, dim = 2)[0, token_length - 1]\n",
    "    symbol_index = int(torch.argmax(prob_last_token, dim =0))\n",
    "    \n",
    "    return symbol_index, float(prob_last_token[symbol_index])\n",
    "\n",
    "\n",
    "\n",
    "def sampling_decode(NNmodel, input_sentence, input_wtoi, output_itow, output_wtoi):\n",
    "    \n",
    "    input_token = torch.tensor(tokenize_input(input_sentence, input_wtoi))\n",
    "    \n",
    "    cur_output_tokens = [output_wtoi['<sos>']]\n",
    "    cur_output = 0\n",
    "    \n",
    "    EOS = output_wtoi['<eos>']\n",
    "    while cur_output != EOS:\n",
    "        cur_output, log_prob = next_symbol(NNmodel, input_token, cur_output_tokens)\n",
    "        cur_output_tokens.append(cur_output)\n",
    "    sentence = detokenize(cur_output_tokens,output_itow)\n",
    "    \n",
    "    return cur_output_tokens, sentence, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46115251"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab_size = len(english_wtoi)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "target_vocab_size = len(telugu_wtoi)\n",
    "model = Translator(input_vocab_size, embedding_dim, hidden_dim, target_vocab_size)\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "n_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 2 ====> train: 6.9497, val: 6.4495\n",
      "Loss at epoch 4 ====> train: 6.7768, val: 7.1361\n",
      "Loss at epoch 6 ====> train: 6.6992, val: 6.4101\n",
      "Loss at epoch 8 ====> train: 6.2812, val: 6.8773\n",
      "Loss at epoch 10 ====> train: 6.1302, val: 6.0598\n",
      "Loss at epoch 12 ====> train: 7.2110, val: 6.7701\n",
      "Loss at epoch 14 ====> train: 6.1419, val: 6.6662\n",
      "Loss at epoch 16 ====> train: 6.8013, val: 6.2881\n",
      "Loss at epoch 18 ====> train: 6.7256, val: 6.6590\n",
      "Loss at epoch 20 ====> train: 6.1398, val: 6.7583\n"
     ]
    }
   ],
   "source": [
    "loss, logits = model_train(train_items, val_items, 20, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 31395, 22796, 3], '<sos> టామ్ చాలా <eos>', 0.025383463129401207)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = preprocess([\"I love you\"]) \n",
    "sampling_decode(model, input_sentence,input_wtoi= english_wtoi, output_itow=telugu_itow, output_wtoi=telugu_wtoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
